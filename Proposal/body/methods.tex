We split this section into four parts: \textbf{Data, Preprocessing, Data Augmentation} and \textbf{Model}.
\subsection{Data}
The dataset included 2000 environmental audio sample(with 40 example per class).
\subsection{Preprocessing}
The data we got is in the form of wave file, so that we can not feed it into CNN directly. We converted environment sound waveform to spectrogram and MFCC. 
These two can seem as visual representation of audio. We often use heatmap to visualize spectrogram and MFCC. X-axis is time, Y-axis is frequency of audio and color shows frequency strength of audio at that moment. 
Most difference of spectrogram and MFCC is that the latter scaled based on human hearing where the former only do log scaled.
\begin{comment}
Spectrogram/ MFCC fig. here
\end{comment}

\subsection{Data Augmentation}
Due to the lack of training sample, we also perform three easy transformation to extend our dataset. 
First is \textbf{white noise}, we add white noise which is a random signal having equal intensity at different frequency as the background.
That make sense because that also lots of noise in real world. Second thing we did is sound shift. Smoothly shift signal in fixed rate. For example, if dog barks at time = 1s, we will shift this event to time = 3s with same amplitude, length\ldots. 
The last augmentation we did is stretching sound, which modify signal sound length to make sound play a little faster.
\subsection{Model}
We used CNN.
